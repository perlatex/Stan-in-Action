---
title: "贝叶斯推断与Stan应用"
author: "王敏杰"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    latex_engine: xelatex
    number_sections: yes
    df_print: kable
linkcolor: red
urlcolor: red
header-includes:
  - \usepackage[fontset = fandol]{ctex}
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{underscore}
  - \usepackage{booktabs}
  - \usepackage{indentfirst}\setlength{\parindent}{2em}
classoptions: "hyperref, 12pt, a4paper"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.show = "hold",
  fig.showtext = TRUE
)
knitr::knit_engines$set(stan = cmdstanr::eng_cmdstan)
```


```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidybayes)
library(ggdist)
library(cmdstanr)
check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)
```


Stan 是当前主流的概率编程语言，主要用于贝叶斯推断，它使用先进的 Hamiltonian Monte Carlo (HMC)采样技术，允许复杂的贝叶斯模型快速收敛，在社会学、生物、医学、物理、工程和商业等领域有广泛的应用。本报告介绍贝叶斯统计推断的数学原理以及Stan应用，并通过案例演示Stan在统计建模中的强大功能。


# 我们今天讲一个数据故事

假定你们校长心血来潮，给你交办了一个任务，让你估算下全校同学的平均身高，你想了想，这好办，来个全校普查，然后统计个均值。但是，很快就发现，这个方法不具备可行性，因为很多同学疫情隔离在家，来不了学校啊，全校普查似乎不现实。这时，统计学院的老师给你出了一个主意，让你随机选取200同学，然后根据这200名同学的身高，推算全校总体的情况。你觉得这个主意不错，很快就拿到了200位同学的身高。数据在这里

```{r, message=FALSE, warning=FALSE}
raw <- 
  readr::read_rds(here::here('data', "height_weight.rds")) %>% 
  as_tibble() %>% 
  mutate(id = 1:n()) %>% 
  mutate(across(c(height, weight), ~ round(.x, 2)))  
  
d <- raw %>% select(id, height)

d %>% 
  head(5) 
```



于是，不费吹灰之力很快就计算出200个同学的平均身高
```{r}
d %>% 
  summarise(mean_height = round(mean(height), 2))
```


马上兴高采烈地报告校长，全校同学的身高均值是164.89。

如果您是校长，您对结果满意？

应该不满意，因为选择的这200个学生，相对全校学生而言，是一个很小的样本，难免以偏概全了。这个164.89可靠性有多高，或者不确定性是多少？你看到校长脸色有些不好看，弱弱地说：“要不我重新再找200个学生，再做一次，或者再找第三组的200个学生，然后第四组，这样重复很多次”，你怕校长不明白你的意思，还把想法画成思维导图

```{r, out.width='75%', fig.align='center', echo = F}
knitr::include_graphics("./images/samplingdistribution_schematic.png")
```



还没等解释完，校长就打断了你的话，“时间来不及了，再说，这又不是做核酸，那有那么多的人力物力，你就用200个同学的身高值，给我估算一个吧，给个范围也行”，最后还不忘提醒一句，“今天就要！”


这下有点头疼了，只有一个样本，还要给出范围，那怎么办呢？


# Bootstrap resampling

方法总是有的，现实不满足的时候，我们只是需要一点点妥协。校长说了，能拿到200个身高记录已经很不错了，那我们就假定我们手头上这唯一样本是随机的，而且能够代表总体分布，这个**假定**就是妥协。接下来那怎么办呢？

搞统计的人发明了一个很不错的方法[Bootstrapping](http://en.wikipedia.org/wiki/Bootstrapping_(statistics))，**有放回的重复抽样**，什么意思呢？我举个通俗的例子：

```{r, out.width='75%', fig.align='center', echo = F}
knitr::include_graphics("./images/bootstrap-sampling-with-replacement.png")
```

- 假定这里有一个口袋，里面装着200个球，你摸一个出来，记录下这个球的重量，然后放回去，搅拌一下，再摸一个出来，称下重量，再放回去，如此往复，记录到200个值后，就停下来，这200个值称之为第一个重抽样样本，然后计算下**均值**。ok，第一个样本的工作完成。
- 然后第二个样本
- 第三个样本...
- 直到1000个重抽样样本，也就得到了1000个均值
- 最后看看这1000个均值的分布




画出思维导图如下
```{r, out.width='75%', fig.align='center', echo = F}
knitr::include_graphics("./images/bootstrapping_schematic.png")
```


道理明白了，那马上开干

```{r}
bootstrap_once <- function(df) {

  boot_idx <- sample(1:nrow(df), replace = TRUE)
  
  df <- df %>% slice(boot_idx)
  
  return(df)
}

bootstrap_repeat <- function(df, reps = 30){
  df_out <- 
    purrr::map_dfr(.x = 1:reps, .f = ~ bootstrap_once(df)) %>% 
    dplyr::mutate(replicate = rep(1:reps, each = nrow(df))) %>% 
    dplyr::group_by(replicate)
  
  return(df_out)
}

```


很快得到了1000个均值
```{r}
df_bootstrap <- d %>% 
  bootstrap_repeat(reps = 1000) %>% 
  group_by(replicate) %>% 
  summarise(mean_height = mean(height)) 

df_bootstrap %>% 
  head(6) 
```


画出了直方图
```{r, out.width = "75%"}
df_bootstrap %>% 
  ggplot(aes(x = mean_height)) +
  geom_histogram(bins = 15, color = "white") +
  ggtitle("Bootstrap sampling distribution")
```



给出置信区间
```{r}
ci <- df_bootstrap %>% 
  ggdist::mean_qi(mean_height) %>% 
  mutate(
    across(where(is.numeric), ~round(.x, 2))
  )
ci
```



置信区间，是频率学的词汇。关于95%的置信区间，要多说两句，我们想统计的是全校同学的身高均值，这是我们的目标，对应到这张图中，这个黑色圆点，它代表着全校同学的平均身高，它是客观存在的，并且有一个确定的值（只是我们不知道）。

```{r, out.width='85%', fig.align='center', echo = F}
knitr::include_graphics("./images/frequentist_confidence_interval.jpg")
```

我们的任务就是，捕获这个黑色圆点，这个过程有点像小朋友玩的**套圈游戏**。我们常说的95%的置信区间，置信区间就是圈圈的大小，95%的意思就是，我们扔套圈100次，95次成功套住黑点，换句话说，扔一次套圈，我有95%的概率能捕获到黑色圆点。当然，如果这个套圈做的特别大，都比场地还大，我们自然可以说，我有100%的概率能套住这个黑点，但没什么意义。 

回到身高问题中来，上面的统计结果告诉我们，我们把套圈设置到`r glue::glue("[{ci$.lower}, {ci$.upper}]")`这个范围，我就有95%的自信说，这个区间能捕获全校同学的平均身高。


需要提醒注意的是，频率学派的思维方式：

- 我们的目标，全体同学的身高均值，是确定的，没有不确定性。
- 用于捕获固定值的置信区间，具有不确定性，因为它可能捕获到，也有可能捕获不到。



# 来点高级的？

以上是频率学派的方法，没有什么问题。但天天吃这个，吃了几十年了，口味难免越吃越高，想搞点新鲜的。
什么是新鲜的呢？

```{r, out.width='85%', fig.align='center', echo = F}
knitr::include_graphics("./images/bayesian_vs_frequentist_meme2.jpg")
```

对的，贝叶斯。我们已经久仰贝叶斯大名。


回到故事的开始，观察到样本数据后，如何推断总体分布的参数$\theta$？


```{r, out.width='85%', fig.align='center', echo = F}
knitr::include_graphics("./images/statistical_inference.png")
```

贝爷用贝叶斯公式，告诉我们可以用贝叶斯后验概率$p(\theta \vert Y)$来回答

```{r, out.width='85%', fig.align='center', echo = F}
knitr::include_graphics("./images/bayesian_theorem.png")
```

三百年前的光辉思想，仍然影响着现在。我们要好好膜拜下这个贝叶斯公式


$$
p(\theta \vert Y) = \frac{p(Y \vert \theta) p(\theta)}{p(Y)}
$$

贝叶斯公式告诉我们，要得到等式的左边，可以用等式的右边来计算。先认识下贝叶斯公式的每个部分。

- 左边 $p(\theta \vert Y)$称之为后验概率，也就是我们的目标
- $p(Y \vert \theta)$ 是似然函数，在给定参数后，数据出现的概率
- $p(\theta)$ 参数的先验概率，在看到数据前，参数各种可能性的分布
- $p(Y)$ 边际似然，可以忽略


既然分母可以先忽略，就认为它为1，于是等式可以变成
$$
p(\theta \vert Y) \propto p(Y \vert \theta) p(\theta).
$$


然后，我们把总体的似然函数，写成每个数据点的似然函数**连乘**的形式：
$$
p(\theta \vert Y) \propto p(\theta) \prod^N_{n=1} p(y_n \vert \theta)
$$


接着，我们两边取对数，连乘变成了连加。也就说，我们计算的是**对数概率(log probabilities)**。使用对数概率可以有效提高数值稳定性

$$
\text{log}\ p(\theta \vert Y) \propto \text{log}\ p(\theta) + \sum^N_{n=1} \text{log}\ p(y_n \vert \theta)
$$

看看这个等式，感觉技术上有可操作性了，没错，它就是贝叶斯数据分析的**灵魂**，我们做贝叶斯计算都是仰仗这个等式。

我们跃跃欲试了，不过，先忘掉之前的统计方法。


## 需要一点前戏

还是校长给出的**身高问题**。通过前面的身高的统计量，我们可以合理的猜测：

- 全校同学的身高均值可能是160，162，170，172，...，或者说这个均值在一个范围之内，在这个范围内，有些值的可能性大，有些值可能性较低。比如，认为这值游离在[150,180]范围，其中168左右的可能最大，两端的可能性最低。如果寻求数学语言来描述，它比较符合正态分布的特征，那就这么定了。

- 方差在[0, 50]范围内都有可能，那就假定每个值的可能性都相等吧。


这两点没有问题，因为符合现实情况，合情合理。现在把我们的猜测画出来，就是这样的，

```{r, fig.width = 6, fig.height = 2.5}
library(patchwork)

p1 <- tibble(
    x = seq(from = 100, to = 230, by = .1),
    y = dnorm(x, mean = 168, sd = 20)
  ) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  xlab("height_mean") +
  ylab("density")


p2 <- 
  tibble(
    x = seq(from = -10, to = 55, by = .1),
    y = dunif(x, min = 0, max = 50)
   ) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  xlab("height_sd") +
  ylab("density")

plot_prior <- p1 + p2
plot_prior
```


## 参数空间

第二步，我们需要构建一个**参数空间**，类似网格一样的东西。具体做法是，先指定参数的范围，大一点没关系，然后把这个范围内的所有**可能参数组合**都罗列出来，类似九九乘法表，比如这里构建 1000x1000 个($\mu$, $\sigma$)参数空间 


```{r out.width = '50%', echo = FALSE}
knitr::include_graphics("images/grid/01grid.png")
```

```{r}
tidyr::crossing(
  mu = 171:177,
  sigma = 2:8
) %>%
  mutate(
    w = glue::glue("N({mu},{sigma})")
  ) %>%
  pivot_wider(
    names_from = sigma,
    values_from = w,
    names_glue = "$\\sigma$ = {sigma}"
  ) %>% 
  mutate(
    mu = glue::glue("$\\mu$ = {mu}")
  ) %>% 
  knitr::kable(format = "latex", escape = FALSE)
```




## 先验概率的对数

第三步，在参数空间里，计算每个参数在先验分布下的概率密度对数，即下面等式红色部分
(不是说均值是正态分布的么，那么160出现的概率是多少，161出现的概率是多少，当然最后求对数)

$$
\text{log}\ p(\theta \vert Y) \propto {\color{red}{\text{log}\ p(\theta)}} + \sum^N_{n=1} \text{log}\ p(y_n \vert \theta)
$$



```{r out.width = '100%', fig.align='left', echo = FALSE}
knitr::include_graphics("images/grid/02prior.png")
```



### 对数似然

第四步，在参数空间里，每个参数组合所对应的分布下，计算观察到的200个身高值的**对数似然**之和，即下面等式红色部分
$$
\text{log}\ p(\theta \vert Y) \propto \text{log}\ p(\theta) + {\color{red}{\sum^N_{n=1} \text{log}\ p(y_n \vert \theta)}}
$$

这里有1000x1000 个($\mu$, $\sigma$)组合，所以会产生 1000x1000 个值

```{r out.width = '100%', fig.align='left', echo = FALSE}
knitr::include_graphics("images/grid/03likelihood.png")
```





### 后验概率的对数

第五步，把先验概率的对数和似然概率的对数加起来, 得到了后验概率对数(log probabilities)，
求指数后，就是**后验概率**

$$
\text{log}\ p(\theta \vert Y) \propto {\color{red}{\text{log}\ p(\theta) + \sum^N_{n=1} \text{log}\ p(y_n \vert \theta)}}
$$

```{r}
d_grid <- crossing(
     mu = seq(from = 150, to = 190, length.out = 1000),
  sigma = seq(from = 4,   to = 9,   length.out = 1000)
)

#head(d_grid)
```


```{r}
grid_function <- function(mu, sigma) {
	dnorm(d$height, mean = mu, sd = sigma, log = T) %>% 
		sum()
}

d_grid <-
	d_grid %>%
	mutate(log_likelihood = map2_dbl(mu, sigma, grid_function)) %>%
	mutate(prior_mu       = dnorm(mu,    mean = 168, sd  = 20, log = T),
	       prior_sigma    = dunif(sigma, min  = 0, max = 50, log = T)) %>%
	mutate(product        = log_likelihood + prior_mu + prior_sigma) %>%
	mutate(probability    = exp(product - max(product)))

#head(d_grid)
```


此时，可以想象成一共有1000 x 1000个坑， 每个坑装着一个后验概率，有高有低，看上去就像若干个小山峰。


```{r, fig.width = 3.25, fig.height = 3, eval=FALSE}
d_grid %>%
  ggplot(aes(x = mu, y = sigma, z = probability)) +
  geom_contour() +
  labs(
    x = expression(mu),
    y = expression(sigma)
  ) +
  coord_cartesian(
    xlim = range(d_grid$mu),
    ylim = range(d_grid$sigma)
  ) +
  theme(panel.grid = element_blank())
```


```{r, fig.width = 4.5, fig.height = 3}
d_grid %>%
  ggplot(aes(x = mu, y = sigma)) +
  geom_raster(
    aes(fill = probability),
    interpolate = T
  ) +
  scale_fill_viridis_c(option = "A") +
  labs(
    x = expression(mu),
    y = expression(sigma)
  ) +
  theme(panel.grid = element_blank())
```






### 抽样

第六步，按照后验概率值的大小抽取样本，得到后验分布。

为什么要抽样呢，因为目前得到的只是概率对数(求指数后是概率)，即每个坑出现的概率，而我们要得到是参数的具体值，身高的均值，所以按照概率大小抽取样本。

```{r}
d_grid_samples <- 
	d_grid %>% 
	sample_n(size = 1e4, replace = T, weight = probability)
```


有了样本，就可以得到均值和标准差的分布

```{r, fig.width = 3.25, fig.height = 3}
d_grid_samples %>% 
	ggplot(aes(x = mu, y = sigma)) + 
	geom_point(size = .9, alpha = 1/15) +
	scale_fill_viridis_c() +
	labs(
	  x = expression(mu[samples]),
		y = expression(sigma[samples])
	) +
	theme(panel.grid = element_blank())
```


```{r, fig.width = 6, fig.height = 2.5}
plot_posterior <- d_grid_samples %>%
	select(mu, sigma) %>%
	pivot_longer(
	  cols = everything()) %>%
	ggplot(aes(x = value)) +
  ggdist::stat_halfeye(
    .width = c(0.66, 0.95), 
    normalize = "panels",
    fill = "skyblue", 
    color = "red"
  ) +
	scale_y_continuous(NULL, breaks = NULL) +
	xlab(NULL) +
	theme(panel.grid = element_blank()) +
	facet_wrap(vars(name), scales = "free")

plot_posterior
```



以及后验概率的**最高密度区间**

```{r}
library(tidybayes)

d_grid_samples %>%
	select(mu, sigma) %>%
	pivot_longer(cols = everything()) %>%
	group_by(name) %>%
	mode_hdi(value, .width = 0.89)
```

此时，给出的不在点估计，而是区间估计。给出了各种可能值，以及各可能值的概率，比频率学给出的信息要丰富很多。


### 后验和先验对比

```{r, fig.asp = 0.816, out.width= "85%"}
plot_prior / plot_posterior
```



### 回望下贝叶斯

回望下贝叶斯的思想

- 我们先赋予参数主观的先验信息，也就意味着参数是变化的值
- 但数据是固定的
- 数据更新了先验信息，得到了后验信息


**贝叶斯推断**符合人们的认知过程。人们总是根据新获得的信息来修正或更新先前的知识或信念。
举个例子，最近有个演员叫李易峰，年轻人长的非常帅，演技又好，又喜欢帮助困难群众，完全是正面的形象，这是我们的先验。或者说是在没有得到进一步信息的之前，我们对这个人物的信念。但他的新闻出来之后，大众就开始修正先前的想法。

我们学习知识的过程，也是贝叶斯的过程。有趣的是，如果你对某个命题有着极强的信念（以100%的先验信念度来表示），
那么不管关于这个命题的新信息（当前概率）如何，贝叶斯定理表明你的后验信念度与先验信念度一样是100%，即你的信念不受新信息的影响。这就叫**已经被洗脑**。


而另一方面，如果你对某个命题没有任何偏见（以50%的先验信念度来表示），
这时如果关于这个命题的新信息给出的当前概率为P%，贝叶斯定理表明你的后验信念度也是P%，即你完全接受新信息。实际生活中，人们总会或多或少受到新信息的影响而修正自己先前的想法（信念）。

好，继续回到我们的问题，这里我们把频率学和贝叶斯两种不同的方法，放在一起，对比一下。

```{r, out.width='85%', fig.align='center', echo = F}
knitr::include_graphics("./images/bayesian_credible_interval.jpg")
```



下图贝叶斯的黑色圆点，是我们的目标。按照贝叶斯的观点，它不是一个固定的或者确定的值，而是各种可能的值，贝叶斯给出的是，**最有可能的是哪些值，以及这些可能值的概率是多少**。
这里用高尔夫球演示，表示的是，很多球都可以打进洞，最有可能落在球洞的是哪些球，以及落入的概率是多少。高尔夫我没玩过，换成足球射门来理解也是一样，位置很偏、角度很刁钻的球，能不能进？也能，但概率很低而已。全校身高150可不可能，也有可能，只是这种可能性很低而已。



以上是通过**网格近似**的方法得到身高分布的后验概率，这个理解起来并不难，但这种方法做起来比较麻烦，需要构建参数网格，对于较复杂的模型，计算量会陡增，内存占用大、比较费时，因此在实际的数据中，一般不采用这种方法，但网格近似的方法可以帮助我们很好地理解贝叶斯数据分析。




# 轮到今天的主角们

##  概率编程工具

- BUGS (Bayesian inference Using Gibbs Sampling)， 2007 年后没有维护
- JAGS (Just Another Gibbs Sampler)
- PyMC  (Python)
- Turing.Jl (Julia)
- [Stan](https://mc-stan.org/) 

前面四个我都没用过，接触到Stan，也纯属偶然，我也只懂一点皮毛。总体感觉Stan比较新，更新比较快（不一定是坏事，可以和它同步成长），Stan的数据结构和R比较接近，还有一点就是它的学习资料相对丰富点，Stan生态要成熟点，有专门的团队维护和答疑。

## 什么是Stan

[Stan](https://mc-stan.org/) 是一门统计编程语言，主要用于贝叶斯推断。
贝叶斯统计已经有近300年的历史，直到最近几十年，得益于理论进步和计算机计算能力的提升，贝叶斯统计才越来越多得到了统计学、其它学科以及工业领域的重视。Stan广泛应用于社会学、生物、医学、物理、工程和商业等领域。也就说，贝叶斯不是新东西，但Stan是新东西。


```{r echo=FALSE, out.width = '30%'}
knitr::include_graphics("./images/stan_logo.png")
```

可以点开主页 <https://mc-stan.org/> 了解

## Stan的历史

```{r echo=FALSE, out.width = '65%'}
knitr::include_graphics("./images/Stanislaw-Ulam.png")
```

波兰犹太裔核物理学家 Stanislaw Ulam (1909-1984)，于美国在第二次世界大战期间，在研究原子弹时，发明了蒙特卡罗(MonteCarlo)方法。蒙特卡罗方法是什么呢? 以概率统计理论为指导的数值计算方法。为什么叫蒙特卡罗呢？据说，因为他的叔叔喜欢去摩纳哥的蒙特卡洛赌场，而且经常输钱。他就研究了赌博的问题，他认为赌城赌钱是概率统计问题。后来他就把他研究的这种方法，叫做蒙特卡罗方法。也就说，用一个赌城的名字，命名指代一种统计方法。总之，蒙特卡罗方法很优秀，所以贝叶斯界用这种方法开发一套程序，就是今天我们讲的这个，并用它创始人的名字Stan命名。

这套程序是由纽约哥伦比亚大学 Andrew Gelman 于2012年发起， 由[核心开发团队](https://mc-stan.org/about/team/)共同开发和维护。



## Stan如何工作

这里面太多数学和计算机的内容了，是核心科技，我真不太懂，求放过。不懂原理，也不用太纠结，Stan 只是一个工具，好比汽车一样，安全驾驶就ok, 至于发动机原理可以先放一放

```{r echo=FALSE, out.width = '65%'}
knitr::include_graphics("./images/qfg.png")
```

## 如何使用Stan

- Stan首先会把Stan代码翻译成C++，然后在本地编译

- Stan 使用先进的采样技术，允许复杂的贝叶斯模型快速收敛，Stan用的是Hamiltonian Monte Carlo技术的 No-U-turn 采样器

- Stan提供了与（R，Python，shell，MATLAB，Julia，Stata）流行语言的接口
   - 在R语言里用rstan，[CmdRstanR](https://mc-stan.org/cmdstanr/index.html) 包，CmdStanR 能随时跟进最新 Stan 的更新

   - 在Python用PyStan包

- 把Stan当作R/Python的一个宏包。一般来说，我们不会单独使用Stan，而是在R语言里完成数据规整变型后，喂给Stan，Stan返回样本后，可以接着在R里统计分析。也就说，**Stan可以当作你已经掌握的数据分析工具的一种插件，当作R语言的一种扩展和增强。** 

- 在R语言里，还可以用bayesplot，tidybayes, loo 等宏包帮助我们完成Stan模型可视化、规整和分析。也就说，R 与 Stan 配合非常默契，简直就是绝配。 




## Stan的优势

相比于传统的方法来说，Stan模型

- 更好的可操作性
  - 从模型表达式到代码，更符合人的直觉
  - 模型灵活性。修改几行代码，就转化成一个新的模型 
 
- 更好的透明性
  - 模型的假设
  - 模型的参数
  
- 更好的可解释性
  - 从贝叶斯公式出发，解释起来更符合常识


什么叫可操作性，我们写出了数学表达，应用到具体案例中，就需要转换成代码，这个转换是最痛苦的。很多同学和我一样，知道了数学模型或者数学表达式，但不知道怎么写代码去实现它，然后去百度R语言代码，依葫芦画瓢地把数据套进去，一运行，有结果了，但是不知道结果什么意思。从这个角度讲，R 语言对新手并不友好， 因为R它认为我们什么都懂，你应该能看懂结果。但事实上，我们还是看不懂。造成这个现状的原因是，**往往用统计的都不是学统计**。Stan语言让**从模型到代码的转化**更加自然，有更好的直觉（这点和tidyverse一样，机器语言接近人类语言，看代码和与人说话一样）。 
同时，可操作性还表现在模型的**灵活性**，一个模型只需要修改几行代码，就可以转化成一个新的模型。 


什么叫透明性？指的是模型结构，比如模型的假设，模型的参数。我们刚才说过，在R语言里面，经常会遇到，代码跑出来了，但是不知道怎么解释模型结果，主要原因还是对模型结构不清楚。**因为**，任何模型都有假设的。不知道模型基于的哪些假设，只拿到一些参数结果，解释起来也自然会很痛苦。比如：

- 线性回归lm()有哪些假设？
- 多层模型lmer()有哪些参数？
- t.test()检验的啥？

这些函数都很强大，但使用也有一些代价，就是需要对它足够了解，否则容易弄错，导致很古怪的解释。 **对于Stan用户而言，就不会有这样的抱怨和困扰，因为模型的结构很清晰**


可解释性？可解释性是上面透明性的延续。一方面，模型结构弄清楚了，模型的参数解释起来就很自然了，另一方面，模型是从贝叶斯公式出发的，融入了先验知识后，从贝叶斯的角度解释参数，比基于频率学的角度，解释起来更符合常识。 
  



## Stan代码框架

Stan语法非常严谨，数据结构接近R语言，声明语句类似C++语言，但我们并不需要搞懂C++后才来学Stan，能看懂R语言，也能看懂Stan，具体可以参考[官方手册](https://mc-stan.org/docs/reference-manual/index.html)。


```{stan, output.var="ex1", eval = FALSE, echo = TRUE}

data{
        // 导入数据
}
parameters{
        // 定义模型要估计的参数
}
model{
        // 后验概率函数
}

```


下面通过一些案例，先让大家了解Stan的强大和价值。


## 从模型到Stan代码

还是全校同学的身高问题，我们可以写成如下表达式

$$
\begin{aligned}
\text{height}_i &\sim \operatorname{normal}(\mu, \,\, \sigma)\\
\mu &  \sim \operatorname{normal}(168, 20)\\
\sigma &\sim \operatorname{half-Cauchy}(0, 1)\\
\end{aligned}
$$



```{r, out.width = '100%', fig.align = 'center', echo = FALSE}
knitr::include_graphics(here::here("images", "from_model_to_code0.png"))
```




```{r, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
stan_program1 <- write_stan_file("
data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  mu ~ normal(168, 20);
  sigma ~ cauchy(0, 1);
  
  y ~ normal(mu, sigma);
}
"
)

stan_data1 <- list(
  N = length(d$height),
  y = d$height
)


model1 <- cmdstan_model(stan_file = stan_program1)
fit1 <- model1$sample(data = stan_data1)
```


```{r, echo=FALSE}
model1$print()
```


啊哈，得到了样本
```{r}
fit1$draws(format = "df") %>% 
  head(16)
```


是最高兴的事情
```{r, out.width='50%', fig.align='center', echo = F}
knitr::include_graphics("./images/samples.png")
```

还是看图过瘾，赶紧画个图吧
```{r, out.width='50%'}
fit1 %>% 
  spread_draws(mu, sigma) %>% 
  ggplot(aes(x = mu)) +
  stat_halfeye(.width = c(0.66, 0.95), fill = "skyblue", color = "red") +
  theme_gray(base_size = 14)
```




```{r}
library(tidybayes)

fit1 %>% 
  gather_draws(mu, sigma) %>%
	mode_hdi(.width = 0.89)
```





# 线性模型

这么好的东西，只用来估计一个均值，太可惜了啊，用它可以搞很多模型啊

还是身高数据，只是我在测量身高的时候，偷偷也测量了体重

```{r}
d <- raw %>% select(id, height, weight)
d %>% head(6) 
```


那我们可以探索身高和体重的关联了

```{r, out.width = '65%'}
d %>% 
  ggplot(aes(x = weight, y = height)) +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE)
```


假定身高和体重满足线性关系，数学表达式如下

$$
\begin{aligned}
\text{height}_i &\sim \operatorname{normal}(\mu_i, \,\, \sigma)\\
\mu_i &= \alpha + \beta \ \text{weight}_i \\
\alpha        & \sim \operatorname{normal}(0, 4) \\
\beta         & \sim \operatorname{normal}(0, 4) \\
\sigma        & \sim \operatorname{half-Cauchy}(1) 
\end{aligned}
$$

我**强烈推荐**这样写，因为，这种写法可以很方便地过渡到其它模型。



## 从模型到Stan代码

```{r, out.width = '100%', fig.align = 'center', echo = FALSE}
knitr::include_graphics(here::here("images", "from_model_to_code.png"))
```




```{r, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
stan_program2 <- write_stan_file("
data {
  int<lower=0> N;
  vector[N] y;
  vector[N] x;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
model {
  y ~ normal(alpha + beta * x, sigma);
  
  alpha  ~ normal(0, 10);
  beta   ~ normal(0, 10);
  sigma  ~ exponential(1);
}

")

stan_data2 <- list(
   N = nrow(d),
   x = d$weight, 
   y = d$height
  )


model2 <- cmdstan_model(stan_file = stan_program2)
fit2 <- model2$sample(data = stan_data2)
```


```{r, echo=FALSE}
model2$print()
```


获得样本后，很容易得到参数的后验分布

```{r, fig.asp = 0.35}
fit2 %>% 
  gather_draws(alpha, beta, sigma) %>% 
	ggplot(aes(x = .value)) +
  ggdist::stat_halfeye(
    .width = c(0.66, 0.95), 
    normalize = "panels",
    fill = "skyblue", 
    color = "red"
  ) +
	facet_wrap(vars(.variable), scales = "free")
```

以及最高密度区间
```{r}
fit2 %>% 
  gather_draws(alpha, beta, sigma) %>% 
	mode_hdi(.width = 0.89)
```






# 多层模型

我们再进一步，不同性别身高和体重的关系，应该是不一样的，我们也探索下吧

```{r}
d <- raw %>% select(id, sex, height, weight)
d %>% head(6)
```


```{r, out.width = '65%'}
d %>% 
  ggplot(aes(x = weight, y = height, color = sex)) +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  scale_color_manual(values = c("boy" = "blue", "girl" = "red")) +
  theme_gray(base_size = 14)
```


这里不是单纯的两个独立的回归分析，而是分成男孩和女孩两组，模型中我们既要考虑组内的变化，又要考虑组与组的之间的变化。因此，多层模型写成如下形式


$$
\begin{aligned}
\text{height}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i         & = \alpha_{j[i]} + \beta_{j[i]} \text{weight}_i \\
\left(\begin{array}
{c}
\alpha_j\\
\beta_j 
\end{array}\right)
&\sim N
\left(\left(\begin{array}
{c}
\mu_{\alpha}\\
\mu_{\beta}
\end{array}\right), 
\left(\begin{array}
{cc}
\sigma_{\alpha}^2 & \rho\sigma_{\alpha}\sigma_{\beta}\\
\rho\sigma_{\alpha}\sigma_{\beta} & \sigma_{\beta}^2
\end{array}\right)
\right)
\end{aligned}
$$


然后加上先验

$$
\begin{aligned}
\mu_\alpha        & \sim \operatorname{Normal}(0, 2) \\
\mu_\beta         & \sim \operatorname{Normal}(0, 2) \\
\sigma        & \sim \operatorname{Exponential}(1) \\
\sigma_\alpha & \sim \operatorname{Exponential}(1) \\
\sigma_\beta  & \sim \operatorname{Exponential}(1) \\
\rho     & \sim \operatorname{LKJcorr}(2)
\end{aligned}
$$

```{r, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
stan_program3 <- write_stan_file("
data {
  int N;                      // number of obs
  int K;                      // number of predictors
  matrix[N, K] X;             // model_matrix
  vector[N] y;                // y
  int J;                      // number of grouping
  int<lower=1, upper=J> g[N]; // index for grouping
}
parameters {
  array[J] vector[K] beta;
  vector[K] MU;
  real<lower=0> sigma;
  
  vector<lower=0>[K] tau;
  corr_matrix[K] Rho;
}
model {
  vector[N] mu;
  sigma ~ exponential(1);
  tau ~ exponential(1);
  Rho ~ lkj_corr(2);
  
  for(i in 1:N) {
    mu[i] = X[i] * beta[g[i]];  
  }
  y ~ normal(mu, sigma); 
  
  beta ~ multi_normal(MU, quad_form_diag(Rho, tau));
}
")


stan_data3 <- d %>% 
  tidybayes::compose_data(
    N = n,
    K = 2,
 		J = n_distinct(sex),
    g = sex,
    y = height,
    X = model.matrix(~ 1 + weight, data = .)
 	)


model3 <- cmdstan_model(stan_file = stan_program3)
fit3 <- model3$sample(data = stan_data3)
```

```{r, echo=FALSE}
model3$print()
```


可以得到男孩和女孩不同的截距和斜率
```{r, echo=FALSE}
fit3$summary(variables = c("beta")) %>% 
  knitr::kable(format = "latex", digits = 3, booktabs = TRUE)
```

给出相关系数

```{r, echo=FALSE}
fit3$summary(variables = c("Rho")) %>% 
  knitr::kable(format = "latex", digits = 3, booktabs = TRUE)
```


# 非线性的案例

我们再来看看，非线性的例子
```{r}
set.seed(1024)
df <- tibble(
  x = runif(n = 10, min = 0, max = 10),
  y = rnorm(n = 10, mean = 5*exp(-0.3*x), sd = 0.5)
)
```

图中的数据点很少，只有10个

```{r, out.width = '65%'}
df %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 3)
```


假定x和y满足下面等式的关系

$$
\begin{aligned}
y_i &= ae^{-bx_i} + \epsilon_i \\
\epsilon_i &\sim \mbox{normal}(0, \; \sigma)
\end{aligned}
$$


写成如下等价这种形式，更好理解

$$
\begin{aligned}
y_i & \sim \operatorname{normal}(\mu_i, \sigma) \\
\mu_i  & = ae^{-bx_i}  \\
\end{aligned}
$$

这里的问题是，如何估计 $a$ 和 $b$ ？

李教授给我布置的作业，是多层和非线性的组合。我们这里的问题要简单很多，但结构上是类似的。




```{r, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
stan_program4 <- write_stan_file("
data {
  int N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real a;
  real b;
  real sigma;
}
model {

  y ~ normal(a * exp(-b * x), sigma);

  a ~ normal(0, 10);
  b ~ normal(0, 10);
  sigma ~ normal(0, 3);
}

generated quantities {
  vector[N] y_rep;
  vector[N] y_fit;
  for (n in 1:N) {
    y_fit[n] = a * exp(-b * x[n]);
    y_rep[n] = normal_rng(a * exp(-b * x[n]), sigma);
  }
}

")


stan_data4 <- df %>%
  tidybayes::compose_data(
    N = nrow(.),
    x = x,
    y = y
  )



model4 <- cmdstan_model(stan_file = stan_program4)
fit4 <- model4$sample(data = stan_data4)
```


```{r, echo=FALSE}
model4$print()
```

## 模型的预测能力
模型推断的好不好呢？是否捕获到数据的特征了呢？我们可以看下模型的预测能力。

```{r}
p1 <- fit4 %>% 
  tidybayes::gather_draws(y_fit[i], ndraws = 200) %>% 
  left_join(df %>% mutate(i = 1:n()), by = "i") %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(data = df, size = 5) +
  geom_line(aes(y = .value, group = .draw), alpha = 0.3, color = "gray") +
  theme_classic(base_size = 14)

p2 <- fit4 %>% 
  tidybayes::gather_draws(y_rep[i]) %>% 
  mean_qi() %>% 
  bind_cols(df) %>% 
  ggplot(aes(x, y)) + 
  geom_point(size = 5) +
  geom_line(aes(y = .value), size = 2, color = "orange") +
  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.3, 
              fill = "gray50"
              ) +
  theme_classic(base_size = 14)
```


```{r, fig.asp = 0.4, out.width= "90%"}
library(patchwork)
p1 + p2
```


一般来说，模型是一种探测手段，用于探测数据的产生机制。我们推断出参数的分布，就可以从后验分布中随机抽取重复样本集。如果一个贝叶斯模型是“好”的，那么从它模拟产生的数据应该与实际观察到的数据很类似。

## 模型对参数的恢复

事实上，数据是我模拟的，真实值$a = 5, b = 0.3$，模型给出的参数估计是
```{r, echo=FALSE}
fit4$summary(variables = c("a", "b")) %>% 
  knitr::kable(format = "latex", digits = 3, booktabs = TRUE)
```

模型捕获和还原了参数

```{r, out.width = '65%'}
true_a_b <- c(a = 5, b = 0.3)
posterior_a_b <- fit4$draws(format = "draws_matrix", variables = c("a", "b"))
bayesplot::mcmc_recover_hist(posterior_a_b, true = true_a_b)
```



# 如何开始

## 配置环境

- 第1步，安装 [R](http://cran.r-project.org)

- 第2步，安装 [Rstudio](https://www.rstudio.com/download)

- 第3步，安装 [Rtools42](https://cran.r-project.org/bin/windows/Rtools/) 到`C:\rtools42`，
(苹果系统不需要这一步)

- 第4步，安装 [CmdStanR](https://mc-stan.org/cmdstanr/index.html)





## 参考书籍

- <https://mc-stan.org/>

- <https://discourse.mc-stan.org/>

- Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. _Bayesian Data Analysis_, Third Edition. Boca Raton: Chapman; Hall/CRC.

- Kruschke, John K. 2014. _Doing Bayesian Data Analysis: A Tutorial Introduction with R_. 2nd Edition. Burlington, MA: Academic Press.

- McElreath, Richard. 2020. _Statistical Rethinking: A Bayesian Course with Examples in R and Stan_. 2nd ed. CRC Texts in Statistical Science. Boca Raton: Taylor; Francis, CRC Press.


# 感谢

感谢 Stan 语言之美!



